// Config settings used for SuperGLUE BERT baseline experiments. 

// This imports the defaults, which can be overridden below.
include "defaults.conf"
exp_name = "mlm"

// Data and preprocessing settings
max_seq_len = 128 // Mainly needed for MultiRC, to avoid over-truncating
                  // But not 512 as that is really hard to fit in memory.
tokenizer = "bert-base-uncased"
// Model settings
input_module = "bert-base-uncased"
bert_embeddings_mode = "top"
load_target_train_checkpoint = "/scratch/gobi2/stephaneao/trained_berts/mlm/best/model_converted.pt"
pair_attn = 0 // shouldn't be needed but JIC
s2s = {
    attention = none
}
pool_type = "first"
sent_enc = "none"
sep_embs_for_skip = 1
classifier = log_reg // following BERT paper
transfer_paradigm = finetune // finetune entire BERT model

// Training settings
dropout = 0.1 // following BERT paper
optimizer = bert_adam
batch_size = 32
max_epochs = 3
lr = .00001
min_lr = .0000001
lr_patience = 4
patience = 20
max_vals = 10000

// Tasks
pretrain_tasks = glue
target_tasks = glue

// Control-flow stuff
do_pretrain = 0
do_target_task_training = 1
do_full_eval = 1
write_preds = "val,test"
write_strict_glue_format = 1

// For WSC 
classifier_loss_fn = "softmax"
classifier_span_pooling = "attn"

// Added args
bert_use_pretrain = 0 // If true, use pretrained model defined by 'bert_model_name' -- ADDED BY STEPHANE
                      // else create bert model using "bert_config_file"
bert_config_file = "/h/stephaneao/sentence_encoders/bert_config.json" // -- ADDED BY STEPHANE
split = 0 // If true, encode each sentence seperately -- ADDED BY STEPHANE
